{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"vae.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPR2A1rQWepTrvwSwXR0sry"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x2645erYUzh_","executionInfo":{"status":"ok","timestamp":1628800787255,"user_tz":-330,"elapsed":2401,"user":{"displayName":"asuran r","photoUrl":"","userId":"13400439017930522440"}},"outputId":"93904f1f-bee3-483a-8142-8fb529228780"},"source":["pip install rdt"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: rdt in /usr/local/lib/python3.7/dist-packages (0.5.0)\n","Requirement already satisfied: scipy<2,>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from rdt) (1.4.1)\n","Requirement already satisfied: pandas<1.1.5,>=1.1 in /usr/local/lib/python3.7/dist-packages (from rdt) (1.1.4)\n","Requirement already satisfied: numpy<2,>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from rdt) (1.19.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<1.1.5,>=1.1->rdt) (2.8.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<1.1.5,>=1.1->rdt) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas<1.1.5,>=1.1->rdt) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LT5nqV_ZUAlk","executionInfo":{"status":"ok","timestamp":1628800794307,"user_tz":-330,"elapsed":5636,"user":{"displayName":"asuran r","photoUrl":"","userId":"13400439017930522440"}}},"source":["from collections import namedtuple\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.mixture import BayesianGaussianMixture\n","\n","from rdt.transformers import OneHotEncodingTransformer\n","\n","import torch\n","from torch.nn import Linear, Module, Parameter, ReLU, Sequential\n","from torch.nn.functional import cross_entropy\n","from torch.optim import Adam\n","from torch.utils.data import DataLoader, TensorDataset"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"q8s6XEU7UI0P","executionInfo":{"status":"ok","timestamp":1628800794311,"user_tz":-330,"elapsed":18,"user":{"displayName":"asuran r","photoUrl":"","userId":"13400439017930522440"}}},"source":["def read_csv(csv_filename, has_header=True, discrete_cols=None):\n","    \"\"\" \"\"\"\n","    data = pd.read_csv(csv_filename, header='infer' if has_header else None)\n","\n","    if discrete_cols:\n","        discrete_columns = discrete_cols.split(',')\n","        if not has_header:\n","            discrete_columns = [int(i) for i in discrete_columns]\n","\n","    else:\n","        discrete_columns = []\n","\n","    return data, discrete_columns"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"txL0eBDIUIxx","executionInfo":{"status":"ok","timestamp":1628800794311,"user_tz":-330,"elapsed":18,"user":{"displayName":"asuran r","photoUrl":"","userId":"13400439017930522440"}}},"source":["\n","AttrMetaDetails = namedtuple(\"AttrMetaDetails\", [\"dim\", \"activation_fn\"])\n","\n","AttrTransformDetails = namedtuple(\n","    \"AttrTransformDetails\", [\"column_name\", \"column_type\",\n","                             \"transformer\", \"transform_aux\",\n","                             \"output_info\", \"output_dimensions\"])\n","\n","\n","class Transformer:\n","    \"\"\"\n","    Each column is transformed independently\n","    Discrete Columns: 1 hot encoded\n","    Continuous Columns: Bayesian Gaussian Mixture\n","                        each value is represented as one hot vector\n","                        indicating the mode (Beta_i,j) and a scalar value (alpha_i,j)\n","                        indicating value within mode.\n","    \"\"\"\n","\n","    def __init__(self, n_modes=10, weightage=0.005):\n","        \"\"\"Create a data transformer.\n","        Args:\n","            n (int):\n","                Maximum number of Gaussian distributions in Bayesian GMM.\n","            weightage (float):\n","                Weight of distribution for a Gaussian distribution to be kept.\n","        \"\"\"\n","        self._n_modes = n_modes\n","        self._weightage = weightage\n","        self._attr_dtypes = []\n","        self._attr_transform_info = []\n","\n","    def fit_discrete(self, column_name, data):\n","        \"\"\" one hot encoder for discrete column.\"\"\"\n","\n","        # ohe = OneHotEncoder()\n","        # ohe.fit(np.array(raw_column_data).reshape(-1,1))\n","        # num_categories = len(ohe.get_feature_names())\n","\n","        ohe = OneHotEncodingTransformer()\n","        ohe.fit(data)\n","        num_categories = len(ohe.dummies)\n","\n","        return AttrTransformDetails(\n","            column_name=column_name, column_type=\"discrete\", transformer=ohe,\n","            transform_aux=None,\n","            output_info=[AttrMetaDetails(num_categories, 'softmax')],\n","            output_dimensions=num_categories)\n","\n","    def transform_discrete(self, attr_transform_details, raw_column_data):\n","        ohe = attr_transform_details.transformer\n","        return [ohe.transform(raw_column_data)]\n","\n","    def fit_continuous(self, column_name, data):\n","        \"\"\" bayesian GMM for continuous column\"\"\"\n","        gm = BayesianGaussianMixture(\n","            self._n_modes,\n","            weight_concentration_prior_type='dirichlet_process',\n","            weight_concentration_prior=0.001,\n","            n_init=2\n","        )\n","\n","        gm.fit(data.reshape(-1, 1))\n","        _weights = gm.weights_\n","        # valid gaussians\n","        valid_component_indicator = _weights > self._weightage\n","        num_components = valid_component_indicator.sum()\n","\n","        return AttrTransformDetails(\n","            column_name=column_name, column_type=\"continuous\", transformer=gm,\n","            transform_aux=valid_component_indicator,\n","            output_info=[AttrMetaDetails(1, 'tanh'), AttrMetaDetails(num_components, 'softmax')],\n","            output_dimensions=1 + num_components)\n","\n","    def transform_continuous(self, attr_transform_details, data):\n","        gmm = attr_transform_details.transformer\n","\n","        valid_component_indicator = attr_transform_details.transform_aux\n","        n_components = valid_component_indicator.sum()\n","\n","        # N(0,1)\n","        means = gmm.means_.reshape((1, self._n_modes))\n","        stds = np.sqrt(gmm.covariances_).reshape((1, self._n_modes))\n","\n","        # alpha_i_j = c_i_j - eta / 4 * phi for_all-> i and valid gaussians\n","        normalized_values = ((data - means) / (4 * stds))[:, valid_component_indicator]\n","\n","        # p_k = pi_k * N_k\n","        component_probs = gmm.predict_proba(data)[:, valid_component_indicator]\n","\n","        # beta\n","        selected_component = np.zeros(len(data), dtype='int')\n","\n","        for i in range(len(data)):\n","            component_prob_t = component_probs[i] + 1e-6\n","            component_prob_t = component_prob_t / component_prob_t.sum()\n","            selected_component[i] = np.random.choice(\n","                np.arange(n_components), p=component_prob_t)\n","\n","        selected_normalized_value = normalized_values[\n","            np.arange(len(data)), selected_component].reshape([-1, 1])\n","\n","        selected_normalized_value = np.clip(selected_normalized_value, -.99, .99)\n","\n","        sel_comp_one_hot_encoded = np.zeros_like(component_probs)\n","\n","        sel_comp_one_hot_encoded[np.arange(len(data)), selected_component] = 1\n","\n","        return [selected_normalized_value, sel_comp_one_hot_encoded]\n","\n","    def fit(self, raw_data, discrete_columns=tuple()):\n","        \"\"\"\n","        fit GMM for continuous columns and One hot encoder for discrete columns.\n","        \"\"\"\n","        self.output_info_list = []\n","        self.output_dimensions = 0\n","\n","        if not isinstance(raw_data, pd.DataFrame):\n","            self.is_dataframe = False\n","            raw_data = pd.DataFrame(raw_data)\n","        else:\n","            self.is_dataframe = True\n","\n","        self._attr_dtypes = raw_data.infer_objects().dtypes\n","\n","        self._attr_transform_info = []\n","\n","        discrete_columns = [raw_data.columns[int(i)] for i in discrete_columns]\n","        for column_name in raw_data.columns:\n","            print(\"Processing column: {}\".format(column_name))\n","            raw_column_data = raw_data[column_name].values\n","            if column_name in discrete_columns:\n","                _transform_info = self.fit_discrete(\n","                    column_name, raw_column_data)\n","            else:\n","                _transform_info = self.fit_continuous(\n","                    column_name, raw_column_data)\n","\n","            self.output_info_list.append(_transform_info.output_info)\n","            self.output_dimensions += _transform_info.output_dimensions\n","            self._attr_transform_info.append(_transform_info)\n","\n","    def transform(self, raw_data):\n","        \"\"\"\"\"\"\n","        if not isinstance(raw_data, pd.DataFrame):\n","            raw_data = pd.DataFrame(raw_data)\n","\n","        column_data_list = []\n","        for attr_transform_info in self._attr_transform_info:\n","            column_data = raw_data[[attr_transform_info.column_name]].values\n","            if attr_transform_info.column_type == \"continuous\":\n","                column_data_list += self.transform_continuous(attr_transform_info, column_data)\n","            else:\n","                assert attr_transform_info.column_type == \"discrete\"\n","                column_data_list += self.transform_discrete(\n","                    attr_transform_info, column_data)\n","\n","        return np.concatenate(column_data_list, axis=1).astype(float)\n","\n","    ###\n","\n","    def inverse_transform_continuous(self, attr_transform_info, attr_data, sigmas, st):\n","        gm = attr_transform_info.transformer\n","        valid_component_indicator = attr_transform_info.transform_aux\n","\n","        sel_normalized_value = attr_data[:, 0]\n","        sel_component_probs = attr_data[:, 1:]\n","\n","        if sigmas is not None:\n","            sig = sigmas[st]\n","            sel_normalized_value = np.random.normal(sel_normalized_value, sig)\n","\n","        sel_normalized_value = np.clip(sel_normalized_value, -1, 1)\n","        comp_probs = np.ones((len(attr_data), self._n_modes)) * -100\n","        comp_probs[:, valid_component_indicator] = sel_component_probs\n","\n","        means = gm.means_.reshape([-1])\n","        stds = np.sqrt(gm.covariances_).reshape([-1])\n","        selected_component = np.argmax(comp_probs, axis=1)\n","\n","        std_t = stds[selected_component]\n","        mean_t = means[selected_component]\n","        column = sel_normalized_value * 4 * std_t + mean_t\n","\n","        return column\n","\n","    def inverse_transform_discrete(self, attr_transform_info, attr_data):\n","        ohe = attr_transform_info.transformer\n","        return ohe.reverse_transform(attr_data)\n","\n","    def inverse_transform(self, data, sigmas=None):\n","        \"\"\"\n","        take matrix data and output raw data.\n","        is_dataframe: pd is_dataframe else numpy array\n","        \"\"\"\n","        st = 0\n","        attr_data_list = []\n","        column_names = []\n","        for attr_transform_info in self._attr_transform_info:\n","            dim = attr_transform_info.output_dimensions\n","            column_data = data[:, st:st + dim]\n","\n","            if attr_transform_info.column_type == 'continuous':\n","                attr_data = self.inverse_transform_continuous(\n","                    attr_transform_info, column_data, sigmas, st)\n","            else:\n","                assert attr_transform_info.column_type == 'discrete'\n","                attr_data = self.inverse_transform_discrete(\n","                    attr_transform_info, column_data)\n","\n","            attr_data_list.append(attr_data)\n","            column_names.append(attr_transform_info.column_name)\n","            st += dim\n","\n","        _data = np.column_stack(attr_data_list)\n","        _data = (pd.DataFrame(_data, columns=column_names).astype(self._attr_dtypes))\n","        if not self.is_dataframe:\n","            _data = _data.values\n","\n","        return _data\n","\n","    def convert_attr_name_val_to_id(self, attr_name, value):\n","        discrete_counter = 0\n","        column_id = 0\n","        for attr_transform_info in self._attr_transform_info:\n","            if attr_transform_info.column_name == attr_name:\n","                break\n","            if attr_transform_info.column_type == \"discrete\":\n","                discrete_counter += 1\n","\n","            column_id += 1\n","\n","        else:\n","            raise ValueError(f\"The column_name `{attr_name}` does not exist in the data.\")\n","\n","        one_hot = attr_transform_info.transform.transform(np.array([value]))[0]\n","        if sum(one_hot) == 0:\n","            raise ValueError(f\"The value `{value}` does not exist in the column `{attr_name}`.\")\n","\n","        return {\n","            \"discrete_col_id\": discrete_counter,\n","            \"column_id\": column_id,\n","            \"value_id\": np.argmax(one_hot)\n","        }\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"GRND-qJEUIu_","executionInfo":{"status":"ok","timestamp":1628800794311,"user_tz":-330,"elapsed":17,"user":{"displayName":"asuran r","photoUrl":"","userId":"13400439017930522440"}}},"source":["class Base:\n","    \"\"\"\n","    Base method\n","    \"\"\"\n","\n","    def save(self, path):\n","        device_backup = self._device\n","        self.set_device(torch.device(\"cpu\"))\n","        torch.save(self, path)\n","        self.set_device(device_backup)\n","\n","    @classmethod\n","    def load(cls, path):\n","        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","        model = torch.load(path)\n","        model.set_device(device)\n","        return model"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"qDZiPbwKUa0U","executionInfo":{"status":"ok","timestamp":1628800794312,"user_tz":-330,"elapsed":18,"user":{"displayName":"asuran r","photoUrl":"","userId":"13400439017930522440"}}},"source":["class Decoder(Module):\n","    def __init__(self, embedding_dim, decompress_dims, data_dim):\n","        super(Decoder, self).__init__()\n","        dim = embedding_dim\n","        seq = []\n","        for item in list(decompress_dims):\n","            seq += [Linear(dim, item), ReLU()]\n","            dim = item\n","\n","        seq.append(Linear(dim, data_dim))\n","        self.seq = Sequential(*seq)\n","        self.sigma = Parameter(torch.ones(data_dim) * 0.1)\n","\n","    def forward(self, input):\n","        return self.seq(input), self.sigma\n","\n","class Encoder(Module):\n","    def __init__(self, data_dim, compress_dims, embedding_dim):\n","        super(Encoder, self).__init__()\n","        dim = data_dim\n","        seq = []\n","        for item in list(compress_dims):\n","            seq += [\n","                Linear(dim, item),\n","                ReLU()\n","            ]\n","            dim = item\n","        self.seq = Sequential(*seq)\n","        self.fc1 = Linear(dim, embedding_dim)\n","        self.fc2 = Linear(dim, embedding_dim)\n","\n","    def forward(self, input):\n","        feature = self.seq(input)\n","        mu = self.fc1(feature)\n","        logvar = self.fc2(feature)\n","        std = torch.exp(0.5 * logvar)\n","        return mu, std, logvar\n","\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"SMGhgJ1cUmk-","executionInfo":{"status":"ok","timestamp":1628800794313,"user_tz":-330,"elapsed":19,"user":{"displayName":"asuran r","photoUrl":"","userId":"13400439017930522440"}}},"source":["class VAE(Base):\n","    \"\"\"\n","\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        embedding_dim=128,\n","        compress_dims=(128, 128),\n","        decompress_dims=(128, 128),\n","        l2scale=1e-5,\n","        batch_size=500,\n","        epochs=1000,\n","        loss_factor=2,\n","        cuda=True\n","    ):\n","\n","        self.embedding_dim = embedding_dim\n","        self.compress_dims = compress_dims\n","        self.decompress_dims = decompress_dims\n","\n","        self.l2scale = l2scale\n","        self.batch_size = batch_size\n","        self.loss_factor = loss_factor\n","        self.epochs = epochs\n","\n","        if not cuda or not torch.cuda.is_available():\n","            device = 'cpu'\n","        elif isinstance(cuda, str):\n","            device = cuda\n","        else:\n","            device = 'cuda'\n","\n","        self._device = torch.device(device)\n","\n","    def fit(self, train_data, discrete_columns=tuple()):\n","        self.transformer = Transformer()\n","        self.transformer.fit(train_data, discrete_columns)\n","        train_data = self.transformer.transform(train_data)\n","        dataset = TensorDataset(torch.from_numpy(train_data.astype('float32')).to(self._device))\n","        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True, drop_last=False)\n","\n","        data_dim = self.transformer.output_dimensions\n","        encoder = Encoder(data_dim, self.compress_dims, self.embedding_dim).to(self._device)\n","        self.decoder = Decoder(self.embedding_dim, self.compress_dims, data_dim).to(self._device)\n","        optimizerAE = Adam(\n","            list(encoder.parameters()) + list(self.decoder.parameters()),\n","            weight_decay=self.l2scale)\n","\n","        for i in range(self.epochs):\n","            print(\"Epoch : {}/{}\".format(i,self.epochs))\n","            for id_, data in enumerate(loader):\n","                optimizerAE.zero_grad()\n","                real = data[0].to(self._device)\n","                mu, std, logvar = encoder(real)\n","                eps = torch.randn_like(std)\n","                emb = eps * std + mu\n","                rec, sigmas = self.decoder(emb)\n","                loss_1, loss_2 = loss_function(\n","                    rec, real, sigmas, mu, logvar,\n","                    self.transformer.output_info_list, self.loss_factor\n","                )\n","                loss = loss_1 + loss_2\n","                loss.backward()\n","                optimizerAE.step()\n","                self.decoder.sigma.data.clamp_(0.01, 1.0)\n","\n","    def sample(self, samples):\n","        self.decoder.eval()\n","\n","        steps = samples // self.batch_size + 1\n","        data = []\n","        for _ in range(steps):\n","            mean = torch.zeros(self.batch_size, self.embedding_dim)\n","            std = mean + 1\n","            noise = torch.normal(mean=mean, std=std).to(self._device)\n","            fake, sigmas = self.decoder(noise)\n","            fake = torch.tanh(fake)\n","            data.append(fake.detach().cpu().numpy())\n","\n","        data = np.concatenate(data, axis=0)\n","        data = data[:samples]\n","        return self.transformer.inverse_transform(data, sigmas.detach().cpu().numpy())\n","\n","    def set_device(self, device):\n","        self._device = device\n","        self.decoder.to(self._device)\n","\n","\n","def loss_function(recon_x, x, sigmas, mu, logvar, output_info, factor):\n","    st = 0\n","    loss = []\n","    for column_info in output_info:\n","        for span_info in column_info:\n","            if span_info.activation_fn != \"softmax\":\n","                ed = st + span_info.dim\n","                std = sigmas[st]\n","                loss.append(((x[:, st] - torch.tanh(recon_x[:, st])) ** 2 / 2 / (std ** 2)).sum())\n","                loss.append(torch.log(std) * x.size()[0])\n","                st = ed\n","\n","            else:\n","                ed = st + span_info.dim\n","                loss.append(cross_entropy(\n","                    recon_x[:, st:ed], torch.argmax(x[:, st:ed], dim=-1), reduction='sum'))\n","                st = ed\n","\n","    assert st == recon_x.size()[1]\n","    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    return sum(loss) * factor / x.size()[0], KLD / x.size()[0]\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"twryZns6VfJt","executionInfo":{"status":"ok","timestamp":1628801120516,"user_tz":-330,"elapsed":947,"user":{"displayName":"asuran r","photoUrl":"","userId":"13400439017930522440"}}},"source":["data, discrete_columns = read_csv('taxi.csv', True, '0,1,2,5,6,7')"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"NH5Rx45hVrJ8","executionInfo":{"status":"ok","timestamp":1628798332089,"user_tz":-330,"elapsed":411,"user":{"displayName":"asuran r","photoUrl":"","userId":"13400439017930522440"}}},"source":["data = data.loc[1:60000,:]"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"hnqd88uBVe-O","executionInfo":{"status":"ok","timestamp":1628801479390,"user_tz":-330,"elapsed":373,"user":{"displayName":"asuran r","photoUrl":"","userId":"13400439017930522440"}}},"source":["model = VAE(epochs = 3000)\n"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"BPMozZxFVy5W","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4c1eb538-ee8f-42ea-9786-0579e73bcd97"},"source":["model.fit(data, tuple(discrete_columns))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Processing column: cat_0\n","Processing column: cat_1\n","Processing column: cat_2\n","Processing column: num_3\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/mixture/_base.py:267: ConvergenceWarning: Initialization 2 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n","  % (init + 1), ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Processing column: num_4\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/mixture/_base.py:267: ConvergenceWarning: Initialization 2 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n","  % (init + 1), ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Processing column: cat_5\n","Processing column: cat_6\n","Processing column: target\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CaopCf3zWmqU","executionInfo":{"status":"ok","timestamp":1628799943697,"user_tz":-330,"elapsed":400,"user":{"displayName":"asuran r","photoUrl":"","userId":"13400439017930522440"}}},"source":["model.save('w3.pt')"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"7VCY6PUNr-mr"},"source":["from google.colab import files"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"TbSYTQWhWjdt","executionInfo":{"status":"ok","timestamp":1628724639943,"user_tz":-330,"elapsed":522,"user":{"displayName":"asuran r","photoUrl":"","userId":"13400439017930522440"}},"outputId":"726796db-7355-4a07-ec95-2e0e11d175cc"},"source":["files.download('w')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_be029213-765d-4436-be21-4b03b4182a7a\", \"w\", 325331)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}}]}]}